{"cells":[{"cell_type":"markdown","metadata":{"id":"Btjy4-h5Gp8O"},"source":["# Import"]},{"cell_type":"markdown","metadata":{"id":"06nQT4xN4Yw7"},"source":["## Import Files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22257,"status":"ok","timestamp":1755938265320,"user":{"displayName":"gy k","userId":"14114798726788534519"},"user_tz":-540},"id":"GE4yAQHe5jnU","outputId":"ede65c8d-023e-47b7-902f-0ed52bdeb374"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4042,"status":"ok","timestamp":1755938269414,"user":{"displayName":"gy k","userId":"14114798726788534519"},"user_tz":-540},"id":"nrY5NWym633Z","outputId":"6f0e9d83-288d-4e71-8129-93f222cb5d9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["'open.zip' ÌååÏùºÏù¥ ÏóÜÏñ¥ Îã§Ïö¥Î°úÎìúÎ•º ÏãúÏûëÌï©ÎãàÎã§.\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1_Xo2vU82JSSadBdD1Kb7iImHnEYdoFGh\n","To: /content/open.zip\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 621k/621k [00:00<00:00, 57.3MB/s]\n"]}],"source":["import gdown\n","import os\n","\n","file_id = '1_Xo2vU82JSSadBdD1Kb7iImHnEYdoFGh'\n","output = 'open.zip' # Ï†ÄÏû•Ìï† ÌååÏùº Ïù¥Î¶Ñ\n","\n","# 'output'ÏúºÎ°ú ÏßÄÏ†ïÎêú ÌååÏùºÏù¥ ÌòÑÏû¨ Í≤ΩÎ°úÏóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùÑ Í≤ΩÏö∞ÏóêÎßå Îã§Ïö¥Î°úÎìú Ïã§Ìñâ\n","if not os.path.exists(output):\n","    print(f\"'{output}' ÌååÏùºÏù¥ ÏóÜÏñ¥ Îã§Ïö¥Î°úÎìúÎ•º ÏãúÏûëÌï©ÎãàÎã§.\")\n","    gdown.download(id=file_id, output=output)\n","else:\n","    print(f\"'{output}' ÌååÏùºÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§. Îã§Ïö¥Î°úÎìúÎ•º Í±¥ÎÑàÎúÅÎãàÎã§.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bn7qrkQ29SJd"},"outputs":[],"source":["# !unzip -qq '/ÌååÏùº Í≤ΩÎ°ú/ÌååÏùºÎ™Ö.zip' -d 'Ï†ÄÏû•Ìï† dir ÏúÑÏπò Í≤ΩÎ°ú'\n","!unzip -qq '/content/open.zip' -d '/content/'"]},{"cell_type":"markdown","metadata":{"id":"SZuHPwZHGsCA"},"source":["## Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60myyOqfGNYD"},"outputs":[],"source":["# =============================================================================\n","# LSTM ÏãúÍ≥ÑÏó¥ ÏàòÏöîÏòàÏ∏° Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ (XGBoost Ï†ÑÎ∂Ä Ï†úÍ±∞ Î≤ÑÏ†Ñ)\n","# - Global Seq2Seq LSTM (Encoder 28 ‚Üí Decoder 7)\n","# - Multi-Head: Î∂ÑÎ•ò(>0 ÌôïÎ•†) + ÌöåÍ∑Ä(log1p Í≥µÍ∞Ñ)\n","# - Soft Gating, Floor, ÏöîÏùº-Ìï© Î¶¨Ïä§ÏºÄÏùº\n","# - Block inference (TEST_00~09) ‚Üí Ï†úÏ∂úÌååÏùº ÏÉùÏÑ±\n","# =============================================================================\n","\n","import os, sys, re, glob, random, math, subprocess\n","from dataclasses import dataclass\n","from typing import List, Dict, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","# -----------------------------------\n","# Optional: holidays ÏÑ§Ïπò Î≥¥Ïû•\n","# -----------------------------------\n","try:\n","    import holidays\n","except ImportError:\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"holidays\", \"-q\"])\n","    import holidays\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"Z_dVtm98Gu2e"},"source":["# Fixed RandomSeed & Setting Hyperparameter"]},{"cell_type":"markdown","metadata":{"id":"WykYlxUmGvqK"},"source":["## Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbE2YGZJGvXM"},"outputs":[],"source":["# =============================================================================\n","# 0) ÏÑ§Ï†ï / Ïú†Ìã∏\n","# =============================================================================\n","\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjZTezKNGzdQ"},"outputs":[],"source":["# ÌïôÏäµ/Î™®Îç∏ Í∏∞Î≥∏Í∞í\n","LOOKBACK = 28     # encoder context\n","PREDICT  = 7      # decoder steps\n","BATCH_SIZE = 256\n","EPOCHS     = 15\n","LR         = 2e-3\n","WEIGHT_DECAY = 1e-4\n","USE_AMP = torch.cuda.is_available()\n","\n","# Í≤åÏù¥ÌåÖ/ÌõÑÏ≤òÎ¶¨\n","ALPHA_GATE       = 0.10    # y_final = y_reg * (p_nonzero ** alpha)\n","FLOOR_RATIO      = None    # floor = pos_quantile(0.45) * ratio\n","GLOBAL_FLOOR_RATIO   = 0.20  # Î©îÎâ¥Î≥Ñ ÏñëÏàò Ï§ëÏïôÍ∞í * ratio\n","DOW_FLOOR_RATIO      = 0.16  # (Î©îÎâ¥,ÏöîÏùº) ÏñëÏàò Ï§ëÏïôÍ∞í * ratio\n","TAU_FLOOR_PROB   = None    # pÍ∞Ä Ïù¥Î≥¥Îã§ ÏûëÏúºÎ©¥ floor ÎØ∏Ï†ÅÏö©\n","RESCALE_CLIP     = (0.9, 2.0)  # ÏöîÏùº-Ìï© Î¶¨Ïä§ÏºÄÏùº clip\n","POS_EPS_PROB    = 0.45     # pÍ∞Ä Ïù¥ Ïù¥ÏÉÅÏù¥Î©¥\n","POS_EPS_FLOOR   = 1.2      # ÏµúÏÜå Œµ=1.0 Î≥¥Ïû• (Ïñ∏Îçî Î∞©ÏßÄ)\n","\n","# Î¶¨Ïä§ÏºÄÏùº Î∞©Ïãù: 'off' | 'per_series' | 'cross_series'\n","RESCALE_MODE = 'cross_series'   # Í∏∞Î≥∏: ÏûêÍ∏∞ ÏãúÍ≥ÑÏó¥Îßå Ï∞∏Ï°∞(ÏïàÏ†Ñ)\n","\n","# ÌïòÎìú Í≤åÏù¥ÌåÖ ÌÜ†Í∏Ä & Í∏∞Î≥∏ œÑ\n","GATE_MODE   = 'off'          # 'off' | 'soft' | 'hard'\n","TAU_DEFAULT = None            # Ï†ÑÏó≠ œÑ (Î©îÎâ¥Î≥Ñ œÑ ÏóÜÏùÑ Îïå ÏÇ¨Ïö©)\n","\n","# Íº¨Î¶¨ Ï∫° (Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò)\n","CAP_Q             = 0.90\n","CAP_MULT          = 1.20\n","APPLY_CAP_AFTER   = True\n","\n","# Í≤ΩÎ°ú\n","TRAIN_PATH   = \"./train/train.csv\"\n","TEST_PATTERN = \"./test/TEST_*.csv\"\n","SAMPLE_SUB   = \"./sample_submission.csv\"\n","SUBMIT_DIR   = \"./submission\"\n","SUBMIT_PATH  = os.path.join(SUBMIT_DIR, \"lstm_submission.csv\")\n","\n","# Ïô∏ÏÉù ÌîºÏ≤ò(ÎØ∏ÎûòÏóê 'Ïïå Ïàò ÏûàÎäî' Í∞íÎßå ÏÇ¨Ïö©)\n","EXO_COLS = [\"weekend\", \"holiday\", \"vacation_season\", \"usage_type_encoded\", \"is_premium\"]"]},{"cell_type":"markdown","metadata":{"id":"awCUrimaG05S"},"source":["# Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dP_9M_w8TaD6"},"outputs":[],"source":["def dataPreProcessing(df: pd.DataFrame) -> pd.DataFrame:\n","    # Ìï≠ÏÉÅ ÏÇ¨Î≥∏ÏúºÎ°ú ÏãúÏûë (view‚Üícopy Í≤ΩÍ≥† Î∞©ÏßÄ)\n","    df = df.copy()\n","\n","    # 1) Ïª¨Îüº Î¶¨ÎÑ§ÏûÑ\n","    if \"ÏòÅÏóÖÏùºÏûê\" in df.columns:\n","        df = df.rename(columns={\n","            \"ÏòÅÏóÖÏùºÏûê\": \"date_time\",\n","            \"ÏòÅÏóÖÏû•Î™Ö_Î©îÎâ¥Î™Ö\": \"market_menu\",\n","            \"Îß§Ï∂úÏàòÎüâ\": \"sales_amount\"\n","        })\n","\n","    # 2) ÎÇ†Ïßú ÌååÏÉù\n","    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n","    df.loc[:, \"weekday\"] = df[\"date_time\"].dt.weekday\n","    df.loc[:, \"weekend\"] = df[\"weekday\"].isin([5, 6]).astype(int)\n","    df.loc[:, \"month\"] = df[\"date_time\"].dt.month\n","    df.loc[:, \"dayofweek\"] = df[\"date_time\"].dt.dayofweek\n","    df.loc[:, \"day\"] = df[\"date_time\"].dt.day\n","\n","    # 3) Í≥µÌú¥Ïùº\n","    import holidays as _hol\n","    kr_holidays = _hol.KR()\n","    df.loc[:, \"holiday\"] = df[\"date_time\"].dt.date.apply(lambda d: int(d in kr_holidays))\n","\n","    # 4) Î∞©Ìïô ÏãúÏ¶å (1,2,7,8Ïõî)\n","    df.loc[:, \"vacation_season\"] = df[\"month\"].isin([1, 2, 7, 8]).astype(int)\n","\n","    # 5) Î©îÎâ¥ Î∂ÑÌï¥ (market_menuÍ∞Ä ÏûàÏùÑ ÎïåÎßå)\n","    if \"market_menu\" in df.columns:\n","        split = df[\"market_menu\"].astype(str).str.split(\"_\", n=1, expand=True)\n","        df.loc[:, \"store_name\"] = split[0]\n","        df.loc[:, \"menu_name\"]  = split[1]\n","\n","    # 6) ÏÇ¨Ïö©Ïú†Ìòï Î∂ÑÎ•ò\n","    def classify_usage_type(menu: str) -> str:\n","        if pd.isna(menu): return \"Í∏∞ÌÉÄ\"\n","        s = str(menu)\n","        if \"Ïñ¥Î¶∞Ïù¥\" in s: return \"Ïñ¥Î¶∞Ïù¥\"\n","        if re.search(r\"Îã®Ï≤¥|ÌîåÎûòÌÑ∞|Î¨¥Ï†úÌïú|[3-9]Ïù∏|Ïù∏Î∂Ñ|ÏÑ∏Ìä∏\", s): return \"Îã®Ï≤¥\"\n","        if \"2Ïù∏\" in s: return \"Ïª§Ìîå\"\n","        if (\"1Ïù∏\" in s) or (\"Îã®Ìíà\" in s) or (\"Gls\" in s): return \"1Ïù∏\"\n","        return \"ÏùºÎ∞ò\"\n","\n","    df.loc[:, \"usage_type\"] = df.get(\"menu_name\", pd.Series(index=df.index)).apply(classify_usage_type)\n","    usage_map = {\"1Ïù∏\":0, \"Ïª§Ìîå\":1, \"Îã®Ï≤¥\":2, \"Ïñ¥Î¶∞Ïù¥\":3, \"ÏùºÎ∞ò\":4, \"Í∏∞ÌÉÄ\":5}\n","    df.loc[:, \"usage_type_encoded\"] = df[\"usage_type\"].map(usage_map).fillna(5).astype(int)\n","\n","    # 7) ÌîÑÎ¶¨ÎØ∏ÏóÑ Ïó¨Î∂Ä\n","    df.loc[:, \"is_premium\"] = df.get(\"menu_name\", pd.Series(index=df.index))\\\n","                                  .astype(str).str.contains(\"ÌïúÏö∞|ÌîÑÎ¶¨ÎØ∏ÏóÑ|ÏàòÏ†ú|ÌäπÏÑ†|ÏôÄÏù∏\", regex=True)\\\n","                                  .fillna(False).astype(int)\n","\n","    # 8) Îß§Ï∂ú Ï†ïÎ¶¨\n","    df.loc[:, \"sales_amount\"] = (\n","        pd.to_numeric(df.get(\"sales_amount\", 0), errors=\"coerce\")\n","          .fillna(0)\n","          .clip(lower=0)\n","          .astype(float)\n","    )\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUtmAf6TW7BB"},"outputs":[],"source":["\n","def build_item_index(train_df: pd.DataFrame) -> Dict[str, int]:\n","    items = sorted(train_df[\"market_menu\"].unique())\n","    item2idx = {it:i for i, it in enumerate(items)}\n","    return item2idx\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuPJisDwG2W2"},"outputs":[],"source":["@torch.no_grad()\n","def fit_tau_by_menu_on_last_block_lstm(train_df, item2idx, model, exo_cols,\n","                                       ctx_len=28, pred_len=7,\n","                                       tau_grid=np.linspace(0.25, 0.60, 8),\n","                                       device=DEVICE):\n","    \"\"\"\n","    Í∞Å Î©îÎâ¥Ïùò train ÎßàÏßÄÎßâ 28‚Üí7 Î∏îÎ°ùÏùÑ Ïù¥Ïö©Ìï¥ 'ÌïòÎìú Í≤åÏù¥ÌåÖ œÑ'Î•º Ï∂îÏ†ï.\n","    Î∞òÌôò: dict {market_menu: best_tau}\n","    \"\"\"\n","    df = train_df.sort_values(['market_menu','date_time']).copy()\n","\n","    tau_map = {}\n","    menus = sorted(df['market_menu'].unique())\n","    fut_cache = {}  # (m, tuple(fut_dates)) caching Ïö©\n","\n","    rows = []\n","    for m in menus:\n","        g = df[df['market_menu']==m].sort_values('date_time')\n","        if len(g) < (ctx_len + pred_len):\n","            continue\n","        # trainÏùò ÎßàÏßÄÎßâ Î∏îÎ°ù Ï†ïÏùò\n","        ctx_end = g['date_time'].max() - pd.Timedelta(days=pred_len)\n","        ctx_start = ctx_end - pd.Timedelta(days=ctx_len - 1)\n","        fut_dates = [ctx_end + pd.Timedelta(days=h) for h in range(1, pred_len+1)]\n","\n","        g_ctx = g[(g['date_time']>=ctx_start)&(g['date_time']<=ctx_end)]\n","        g_tgt = g[(g['date_time']>ctx_end)&(g['date_time']<=ctx_end+pd.Timedelta(days=pred_len))]\n","\n","        if len(g_ctx)!=ctx_len or len(g_tgt)!=pred_len:\n","            continue  # Î∂àÏôÑÏ†Ñ Î∏îÎ°ù skip (ÎìúÎ¨æ)\n","        # ÌÖêÏÑú Ï§ÄÎπÑ\n","        y_ctx = torch.tensor(np.log1p(g_ctx['sales_amount'].to_numpy(dtype=np.float32)),\n","                             dtype=torch.float32, device=device).unsqueeze(0)          # [1,28]\n","        x_ctx = torch.tensor(g_ctx[exo_cols].to_numpy(dtype=np.float32),\n","                             dtype=torch.float32, device=device).unsqueeze(0)          # [1,28,F]\n","        # ÎØ∏Îûò exo\n","        fut = pd.DataFrame({'date_time': fut_dates, 'market_menu':[m]*pred_len, 'sales_amount':0.0})\n","        fut = dataPreProcessing(fut)\n","        x_tgt = torch.tensor(fut[exo_cols].to_numpy(dtype=np.float32),\n","                             dtype=torch.float32, device=device).unsqueeze(0)          # [1,7,F]\n","        item_idx = torch.tensor([item2idx.get(m, len(item2idx))], dtype=torch.long, device=device)\n","\n","        # Î™®Îç∏ Ï∂îÎ°†\n","        model.eval()\n","        logits, reg = model(item_idx, y_ctx, x_ctx, x_tgt, teacher_forcing=False)\n","        p = torch.sigmoid(logits).cpu().numpy().reshape(-1)          # [7]\n","        y_reg = torch.expm1(reg).clamp_min(0.0).cpu().numpy().reshape(-1)   # [7]\n","        y_true = g_tgt['sales_amount'].to_numpy(dtype=np.float32)    # [7]\n","\n","        # œÑ ÌÉêÏÉâ\n","        best_s, best_tau = 1e9, TAU_DEFAULT\n","        for tau in tau_grid:\n","            y_hat = np.where(p < tau, 0.0, y_reg).clip(0.0)\n","            s = smape_np(y_true, y_hat)\n","            if s < best_s:\n","                best_s, best_tau = s, float(tau)\n","        tau_map[m] = best_tau\n","\n","    print(f\"œÑ ÌïôÏäµ ÏôÑÎ£å: {len(tau_map)}Í∞ú Î©îÎâ¥\")  # ÏùºÎ∂Ä ÏßßÏùÄ ÏãúÎ¶¨Ï¶àÎäî Ï†úÏô∏Îê† Ïàò ÏûàÏùå\n","    return tau_map\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKdBtwjDTXoT"},"outputs":[],"source":["# =============================================================================\n","# 2) PyTorch Dataset\n","#  - Í∞Å itemÏùò Ï†ÑÏ≤¥ Í∏∞Í∞ÑÏùÑ ÌõëÏúºÎ©∞, (LOOKBACK=28 ‚Üí PREDICT=7) ÏúàÎèÑÏö∞ ÏÉùÏÑ±\n","#  - Í≤ÄÏ¶ùÏùÄ \"Í∞Å itemÏùò ÎßàÏßÄÎßâ Ìïú ÏúàÎèÑÏö∞\"Îßå ÏÇ¨Ïö©\n","# =============================================================================\n","class LSTMDataset(Dataset):\n","    def __init__(self,\n","                 df: pd.DataFrame,\n","                 item2idx: Dict[str,int],\n","                 exo_cols: List[str],\n","                 ctx_len=28, pred_len=7,\n","                 split=\"train\"):\n","        \"\"\"\n","        df: columns = [market_menu, date_time, sales_amount, ... exo_cols ...]\n","        split: \"train\" | \"val\"\n","          - train: Í∞Å itemÏùò ÎßàÏßÄÎßâ ÏúàÎèÑÏö∞Î•º Ï†úÏô∏Ìïú Î™®Îì† ÏúàÎèÑÏö∞\n","          - val:   Í∞Å itemÏùò ÎßàÏßÄÎßâ ÏúàÎèÑÏö∞ 1Í∞úÎßå\n","        \"\"\"\n","        self.df = df.sort_values([\"market_menu\",\"date_time\"]).reset_index(drop=True)\n","        self.item2idx = item2idx\n","        self.exo_cols = exo_cols\n","        self.ctx_len = ctx_len\n","        self.pred_len = pred_len\n","        self.split = split\n","\n","        self.index: List[Tuple[np.ndarray, np.ndarray]] = []\n","        self.items: List[int] = []\n","\n","        for item, g in self.df.groupby(\"market_menu\", sort=False):\n","            n = len(g)\n","            total = ctx_len + pred_len\n","            if n < total:  # ÏúàÎèÑÏö∞ ÏÉùÏÑ± Î∂àÍ∞Ä\n","                continue\n","            # ÎßàÏßÄÎßâ ÏúàÎèÑÏö∞ ÏãúÏûë ÏúÑÏπò\n","            s_val = n - total\n","            if split == \"val\":\n","                starts = [s_val]\n","            else:  # train\n","                starts = list(range(0, s_val))  # ÎßàÏßÄÎßâ ÌïòÎÇò ÎπºÍ≥† Î™®Îëê ÌïôÏäµ\n","\n","            for s in starts:\n","                ctx_idx = g.index[s:s+ctx_len].to_numpy()\n","                tgt_idx = g.index[s+ctx_len:s+total].to_numpy()\n","                self.index.append((ctx_idx, tgt_idx))\n","                self.items.append(self.item2idx.get(item, -1))\n","\n","        self.items = np.array(self.items, dtype=np.int64)\n","\n","    def __len__(self):\n","        return len(self.index)\n","\n","    def __getitem__(self, i: int):\n","        ctx_idx, tgt_idx = self.index[i]\n","        item_idx = self.items[i]\n","        # unknown Î≥¥Ìò∏\n","        if item_idx < 0:\n","            item_idx = len(self.item2idx)  # UNK id\n","\n","        ctx = self.df.loc[ctx_idx]\n","        tgt = self.df.loc[tgt_idx]\n","\n","        # log1p scale\n","        y_ctx = torch.tensor(np.log1p(ctx[\"sales_amount\"].to_numpy()), dtype=torch.float32)\n","        y_tgt = torch.tensor(np.log1p(tgt[\"sales_amount\"].to_numpy()), dtype=torch.float32)\n","        y_tgt_bin = torch.tensor((tgt[\"sales_amount\"].to_numpy() > 0).astype(np.float32))\n","\n","        x_ctx = torch.tensor(ctx[self.exo_cols].to_numpy(), dtype=torch.float32)\n","        x_tgt = torch.tensor(tgt[self.exo_cols].to_numpy(), dtype=torch.float32)\n","\n","        return {\n","            \"item_idx\": torch.tensor(item_idx, dtype=torch.long),\n","            \"y_ctx\": y_ctx, \"x_ctx\": x_ctx,\n","            \"y_tgt\": y_tgt, \"y_tgt_bin\": y_tgt_bin,\n","            \"x_tgt\": x_tgt\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81zji3NdC9fk"},"outputs":[],"source":["\n","\n","\n","# =============================================================================\n","# 3) Î™®Îç∏: Global Seq2Seq LSTM + Multi-Head\n","# =============================================================================\n","class Seq2SeqLSTM(nn.Module):\n","    def __init__(self, n_items: int, exo_dim: int,\n","                 emb_dim=32, hid=192, num_layers=2, dropout=0.1):\n","        super().__init__()\n","        self.item_emb = nn.Embedding(n_items, emb_dim)\n","\n","        enc_in = 1 + exo_dim + emb_dim\n","        dec_in = 1 + exo_dim + emb_dim\n","\n","        self.encoder = nn.LSTM(enc_in, hid, num_layers=num_layers,\n","                               batch_first=True, dropout=dropout)\n","        self.decoder = nn.LSTM(dec_in, hid, num_layers=num_layers,\n","                               batch_first=True, dropout=dropout)\n","\n","        self.clf_head = nn.Linear(hid, 1)  # logits\n","        self.reg_head = nn.Linear(hid, 1)  # log1p scale\n","\n","    def forward(self, item_idx, y_ctx, x_ctx, x_tgt, y_tgt=None, teacher_forcing=True):\n","        \"\"\"\n","        item_idx: [B]\n","        y_ctx:    [B, 28]\n","        x_ctx:    [B, 28, F]\n","        x_tgt:    [B,  7, F]\n","        y_tgt:    [B,  7] (optional; log1p)\n","        \"\"\"\n","        B, Tctx = y_ctx.shape\n","        _, Ttgt, Fexo = x_tgt.shape\n","\n","        emb = self.item_emb(item_idx)              # [B, E]\n","        emb_ctx = emb.unsqueeze(1).repeat(1, Tctx, 1)\n","        enc_in = torch.cat([y_ctx.unsqueeze(-1), x_ctx, emb_ctx], dim=-1)  # [B,28,1+F+E]\n","\n","        _, (h, c) = self.encoder(enc_in)\n","\n","        y_prev = y_ctx[:, -1].unsqueeze(-1)  # [B,1]\n","        logits, reg = [], []\n","\n","        for t in range(Ttgt):\n","            x_step = x_tgt[:, t, :]          # [B, F]\n","            dec_in_t = torch.cat([y_prev, x_step, emb], dim=-1)  # [B, 1+F+E]\n","            out, (h, c) = self.decoder(dec_in_t.unsqueeze(1), (h, c))  # out: [B,1,H]\n","            out = out.squeeze(1)\n","\n","            logit_t = self.clf_head(out)   # [B,1]\n","            reg_t   = self.reg_head(out)   # [B,1]\n","            logits.append(logit_t)\n","            reg.append(reg_t)\n","\n","            if teacher_forcing and (y_tgt is not None):\n","                y_prev = y_tgt[:, t].unsqueeze(-1)  # log1p\n","            else:\n","                y_prev = reg_t  # autoregressive in log1p space\n","\n","        logits = torch.cat(logits, dim=1)   # [B,7]\n","        reg    = torch.cat(reg, dim=1)      # [B,7]\n","        return logits, reg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPUjTyFqZaUY"},"outputs":[],"source":["# =============================================================================\n","# 4) ÌïôÏäµ/ÌèâÍ∞Ä Î£®ÌîÑ\n","# =============================================================================\n","def smape_np(y_true: np.ndarray, y_pred: np.ndarray, eps=1e-6) -> float:\n","    y_true = y_true.astype(float)\n","    y_pred = y_pred.astype(float)\n","    den = (np.abs(y_true) + np.abs(y_pred)).clip(min=eps)\n","    num = np.abs(y_true - y_pred)\n","    return float((2.0 * num / den).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFp_4mNuFQHF"},"outputs":[],"source":["def train_epoch(model, loader, opt, scaler=None, alpha_gate=0.35, device=DEVICE):\n","    model.train()\n","    total = 0.0\n","    for batch in tqdm(loader, leave=False):\n","        for k in batch:\n","            batch[k] = batch[k].to(device)\n","\n","        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=USE_AMP):\n","            logits, reg = model(batch[\"item_idx\"],\n","                                batch[\"y_ctx\"], batch[\"x_ctx\"],\n","                                batch[\"x_tgt\"], batch[\"y_tgt\"],\n","                                teacher_forcing=True)\n","\n","            p = torch.sigmoid(logits)                 # [B,7]\n","            y_reg = torch.expm1(reg).clamp_min(0.0)   # [B,7]\n","            y_hat = y_reg * (p ** alpha_gate)\n","\n","            y_true = torch.expm1(batch[\"y_tgt\"]).clamp_min(0.0)\n","            y_bin  = batch[\"y_tgt_bin\"]\n","\n","            loss_clf = F.binary_cross_entropy_with_logits(logits, y_bin)\n","            loss_reg = F.l1_loss(reg, batch[\"y_tgt\"])     # log1p MAE\n","            smape = (2.0 * (y_hat - y_true).abs() / (y_hat.abs() + y_true.abs() + 1e-6)).mean()\n","            loss = loss_clf + loss_reg + 0.2 * smape\n","\n","        opt.zero_grad(set_to_none=True)\n","        if scaler is not None:\n","            scaler.scale(loss).backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            scaler.step(opt)\n","            scaler.update()\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","\n","        total += float(loss.detach().cpu().item())\n","    return total / max(1, len(loader))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEnTocy5FV38"},"outputs":[],"source":["@torch.no_grad()\n","def eval_epoch(model, loader, alpha_gate=0.35, device=DEVICE):\n","    model.eval()\n","    smapes = []\n","    for batch in loader:\n","        for k in batch:\n","            batch[k] = batch[k].to(device)\n","        logits, reg = model(batch[\"item_idx\"],\n","                            batch[\"y_ctx\"], batch[\"x_ctx\"],\n","                            batch[\"x_tgt\"], teacher_forcing=False)\n","        p = torch.sigmoid(logits)\n","        y_reg = torch.expm1(reg).clamp_min(0.0)\n","        y_hat = y_reg * (p ** alpha_gate)\n","        y_true = torch.expm1(batch[\"y_tgt\"]).clamp_min(0.0)\n","\n","        s = (2.0 * (y_hat - y_true).abs() / (y_hat.abs() + y_true.abs() + 1e-6)).mean()\n","        smapes.append(float(s.detach().cpu().item()))\n","    return float(np.mean(smapes)) if smapes else 999.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkTE22OqyE8Y"},"outputs":[],"source":["def run_train_lstm(train_df: pd.DataFrame,\n","                   item2idx: Dict[str,int],\n","                   exo_cols: List[str],\n","                   ctx_len=28, pred_len=7,\n","                   batch_size=256, epochs=15, lr=2e-3,\n","                   device=DEVICE):\n","    # Dataset / Loader\n","    ds_tr  = LSTMDataset(train_df, item2idx, exo_cols, ctx_len, pred_len, split=\"train\")\n","    ds_val = LSTMDataset(train_df, item2idx, exo_cols, ctx_len, pred_len, split=\"val\")\n","\n","    loader_tr  = DataLoader(ds_tr,  batch_size=batch_size, shuffle=True,  drop_last=True,  num_workers=0)\n","    loader_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0)\n","\n","    n_items = len(item2idx) + 1  # +1 for UNK\n","    model = Seq2SeqLSTM(n_items=n_items, exo_dim=len(exo_cols),\n","                        emb_dim=32, hid=192, num_layers=2, dropout=0.1).to(device)\n","\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n","    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n","\n","    best = 999.0\n","    best_state = None\n","    for ep in range(1, epochs+1):\n","        tr = train_epoch(model, loader_tr, opt, scaler, alpha_gate=ALPHA_GATE, device=device)\n","        va = eval_epoch(model, loader_val, alpha_gate=ALPHA_GATE, device=device)\n","        print(f\"[EP {ep:02d}] train_loss={tr:.4f}  val_sMAPE={va:.5f}\")\n","        if va < best:\n","            best = va\n","            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    return model, best"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpcfJ_ekyYN2"},"outputs":[],"source":["@torch.no_grad()\n","def forecast_next7_for_block_lstm(\n","    raw_test_df: pd.DataFrame,\n","    item2idx: dict,\n","    model,\n","    exo_cols,\n","    alpha=ALPHA_GATE,\n","    global_floor_ratio=GLOBAL_FLOOR_RATIO,\n","    dow_floor_ratio=DOW_FLOOR_RATIO,\n","    pos_eps_prob=POS_EPS_PROB,\n","    pos_eps_floor=POS_EPS_FLOOR,\n","    rescale_mode=RESCALE_MODE,\n","    rescale_clip=RESCALE_CLIP,\n","    # ‚òÖ Ï∂îÍ∞Ä: Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÏÉÅÎã® Ï∫°\n","    cap_q: float = CAP_Q,\n","    cap_mult: float = CAP_MULT,\n","    apply_cap_after: bool = APPLY_CAP_AFTER,\n","    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    block_id=\"TEST_XX\",\n","):\n","    T = dataPreProcessing(raw_test_df).sort_values([\"market_menu\",\"date_time\"]).copy()\n","    menus = T[\"market_menu\"].drop_duplicates().tolist()\n","    unk_id = len(item2idx)\n","\n","    ctx_last  = T[\"date_time\"].max()\n","    ctx_start = ctx_last - pd.Timedelta(days=LOOKBACK-1)\n","    fut_dates = [ctx_last + pd.Timedelta(days=h) for h in range(1, PREDICT+1)]\n","    ctx_hist  = T[(T[\"date_time\"] >= ctx_start) & (T[\"date_time\"] <= ctx_last)].copy()\n","    ctx_hist[\"dow\"] = ctx_hist[\"date_time\"].dt.dayofweek\n","\n","    # ‚îÄ‚îÄ Î∞îÎã•Í∞í\n","    gpos = ctx_hist[ctx_hist[\"sales_amount\"]>0]\n","    med_by_menu = gpos.groupby(\"market_menu\")[\"sales_amount\"].median().reindex(menus).fillna(0.0)\n","    global_floor_map = (global_floor_ratio * med_by_menu).to_dict() if (global_floor_ratio or 0)>0 else {}\n","\n","    med_by_menu_dow = (gpos.groupby([\"market_menu\",\"dow\"])[\"sales_amount\"].median())\n","    dow_floor_map = {k: dow_floor_ratio*float(v) for k, v in med_by_menu_dow.to_dict().items()} \\\n","                    if (dow_floor_ratio or 0)>0 else {}\n","\n","    # Î¶¨Ïä§ÏºÄÏùº ÌÉÄÍπÉ\n","    per_series_target = (ctx_hist.groupby([\"market_menu\",\"dow\"])[\"sales_amount\"].median().to_dict())\n","    cross_target_by_dow = {}\n","    if rescale_mode == 'cross_series':\n","        for d in range(7):\n","            sums = (ctx_hist[ctx_hist[\"dow\"]==d].groupby(\"date_time\")[\"sales_amount\"].sum())\n","            cross_target_by_dow[d] = float(sums.median()) if len(sums) else None\n","\n","    # Î∞∞Ïπò ÌÖêÏÑú\n","    y_ctx_list, x_ctx_list, x_tgt_list, item_idx_list = [], [], [], []\n","    for m in menus:\n","        g = T[T[\"market_menu\"]==m].sort_values(\"date_time\")\n","        g_ctx = g.tail(LOOKBACK)\n","        if len(g_ctx) != LOOKBACK:\n","            raise AssertionError(f\"{block_id}/{m}: expected {LOOKBACK} days, got {len(g_ctx)}\")\n","        y_ctx_list.append(np.log1p(g_ctx[\"sales_amount\"].to_numpy(np.float32)))\n","        x_ctx_list.append(g_ctx[exo_cols].to_numpy(np.float32))\n","\n","        fut = pd.DataFrame({'date_time': fut_dates, 'market_menu':[m]*PREDICT, 'sales_amount':0.0})\n","        fut = dataPreProcessing(fut)\n","        x_tgt_list.append(fut[exo_cols].to_numpy(np.float32))\n","\n","        item_idx_list.append(item2idx.get(m, unk_id))\n","\n","    y_ctx  = torch.tensor(np.stack(y_ctx_list), dtype=torch.float32, device=device)\n","    x_ctx  = torch.tensor(np.stack(x_ctx_list), dtype=torch.float32, device=device)\n","    x_tgt  = torch.tensor(np.stack(x_tgt_list), dtype=torch.float32, device=device)\n","    it_idx = torch.tensor(np.array(item_idx_list), dtype=torch.long,   device=device)\n","\n","    # Î™®Îç∏ Ï∂îÎ°† + ÏÜåÌîÑÌä∏ Í≤åÏù¥ÌåÖ\n","    model.eval()\n","    logits, reg = model(it_idx, y_ctx, x_ctx, x_tgt, teacher_forcing=False)\n","    p     = torch.sigmoid(logits).cpu().numpy()\n","    y_reg = torch.expm1(reg).clamp_min(0.0).cpu().numpy()\n","    y     = y_reg * np.power(p, alpha)\n","\n","    # anti-zero Î∞îÎã•Í∞í\n","    if global_floor_map:\n","        gfloor = np.zeros_like(y)\n","        for i, m in enumerate(menus): gfloor[i, :] = float(global_floor_map.get(m, 0.0))\n","        y = np.maximum(y, gfloor)\n","    if dow_floor_map:\n","        for i, m in enumerate(menus):\n","            for h, fut_dt in enumerate(fut_dates):\n","                y[i, h] = max(y[i, h], dow_floor_map.get((m, fut_dt.dayofweek), 0.0))\n","    if (pos_eps_prob is not None) and (pos_eps_floor is not None) and pos_eps_floor>0:\n","        strong = (p >= pos_eps_prob)\n","        y = np.where(strong, np.maximum(y, pos_eps_floor), y)\n","\n","    # Î¶¨Ïä§ÏºÄÏùº\n","    if rescale_mode == 'per_series':\n","        for i, m in enumerate(menus):\n","            for h, fut_dt in enumerate(fut_dates):\n","                tgt = per_series_target.get((m, fut_dt.dayofweek), None)\n","                if tgt is None: continue\n","                k = np.clip(tgt / (y[i, h] + 1e-6), rescale_clip[0], rescale_clip[1])\n","                y[i, h] = (y[i, h] * k).clip(0.0)\n","    elif rescale_mode == 'cross_series':\n","        for h, fut_dt in enumerate(fut_dates):\n","            tgt = cross_target_by_dow.get(fut_dt.dayofweek, None)\n","            if tgt is None: continue\n","            k = np.clip(tgt / (float(y[:, h].sum()) + 1e-6), rescale_clip[0], rescale_clip[1])\n","            y[:, h] = (y[:, h] * k).clip(0.0)\n","\n","    # ‚òÖ Î¶¨Ïä§ÏºÄÏùº Ïù¥ÌõÑ (Î©îÎâ¥,ÏöîÏùº) q-ÏÉÅÌïú Ï†ÅÏö©\n","    if apply_cap_after and (cap_q is not None) and (cap_mult is not None):\n","        cap_map = {}\n","        gp = ctx_hist[ctx_hist[\"sales_amount\"]>0]\n","        if not gp.empty:\n","            q = gp.groupby([\"market_menu\",\"dow\"])[\"sales_amount\"].quantile(cap_q)\n","            cap_map = {k: float(v)*cap_mult for k, v in q.to_dict().items()}\n","        if cap_map:\n","            for i, m in enumerate(menus):\n","                for h, fut_dt in enumerate(fut_dates):\n","                    c = cap_map.get((m, fut_dt.dayofweek), None)\n","                    if c is not None and y[i, h] > c: y[i, h] = c\n","\n","    # Í≤∞Í≥º DF\n","    rows = []\n","    for i, m in enumerate(menus):\n","        for h in range(PREDICT):\n","            rows.append({\"token\": f\"{block_id}+{h+1}Ïùº\", \"ÏòÅÏóÖÏû•Î™Ö_Î©îÎâ¥Î™Ö\": m, \"Îß§Ï∂úÏàòÎüâ\": float(y[i, h])})\n","    return pd.DataFrame(rows)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUS3xCzfyeeb"},"outputs":[],"source":["def infer_all_blocks_lstm(test_glob_pattern: str,\n","                          item2idx: Dict[str,int],\n","                          model: Seq2SeqLSTM,\n","                          exo_cols: List[str],\n","                          alpha=ALPHA_GATE,\n","                          global_floor_ratio=GLOBAL_FLOOR_RATIO,\n","                          dow_floor_ratio=DOW_FLOOR_RATIO,\n","                          pos_eps_prob=POS_EPS_PROB,\n","                          pos_eps_floor=POS_EPS_FLOOR,\n","                          rescale_mode=RESCALE_MODE,\n","                          rescale_clip=RESCALE_CLIP,\n","                          cap_q=CAP_Q,\n","                          cap_mult=CAP_MULT,\n","                          apply_cap_after=APPLY_CAP_AFTER,\n","                          decay=None,\n","                          device=DEVICE) -> pd.DataFrame:\n","    files = sorted(glob.glob(test_glob_pattern))\n","    all_preds = []\n","    for path in files:\n","        raw = pd.read_csv(path)\n","        block_id = os.path.splitext(os.path.basename(path))[0]\n","        dfp = forecast_next7_for_block_lstm(\n","            raw_test_df=raw,\n","            item2idx=item2idx,\n","            model=model,\n","            exo_cols=exo_cols,\n","            alpha=alpha,\n","            global_floor_ratio=global_floor_ratio,\n","            dow_floor_ratio=dow_floor_ratio,\n","            pos_eps_prob=pos_eps_prob,\n","            pos_eps_floor=pos_eps_floor,\n","            rescale_mode=rescale_mode,\n","            rescale_clip=rescale_clip,\n","            cap_q=cap_q, cap_mult=cap_mult, apply_cap_after=apply_cap_after,\n","            device=device, block_id=block_id\n","        )\n","        if decay is not None:\n","            h = dfp['token'].str.extract(r'\\+(\\d)Ïùº')[0].astype(int)\n","            dfp['Îß§Ï∂úÏàòÎüâ'] = (dfp['Îß§Ï∂úÏàòÎüâ'] * h.map(decay).fillna(1.0)).clip(lower=0)\n","        all_preds.append(dfp)\n","    return pd.concat(all_preds, ignore_index=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E4c8pBKkG4Nh"},"source":["# Define Model"]},{"cell_type":"markdown","metadata":{"id":"I2aoxBRCG5ai"},"source":["## Baseline"]},{"cell_type":"markdown","metadata":{"id":"4Hp7H3WzG7S8"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"sHXYrHOyHEfW"},"source":["## Baseline"]},{"cell_type":"markdown","metadata":{"id":"WYIBpgo7G8m8"},"source":["# Prediction"]},{"cell_type":"markdown","metadata":{"id":"FkI6JV21HdWy"},"source":["## Baseline"]},{"cell_type":"markdown","metadata":{"id":"JvCVomTDZzVd"},"source":["## gagyeomkim"]},{"cell_type":"markdown","metadata":{"id":"5PvM_N-ALaVI"},"source":["## KGY"]},{"cell_type":"markdown","metadata":{"id":"Jshgqe-EG-P7"},"source":["# Submission"]},{"cell_type":"markdown","metadata":{"id":"D0EhDF9THmWl"},"source":["## Baseline"]},{"cell_type":"markdown","metadata":{"id":"9B4l-JdeMZfV"},"source":["### KGY VERSION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17dzCgZLLDkA"},"outputs":[],"source":["# =============================================================================\n","# 6) Ï†úÏ∂ú ÏÉùÏÑ±\n","# =============================================================================\n","def build_submission(full_pred_df: pd.DataFrame,\n","                     sample_submission_path: str,\n","                     save_path: str = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    sample_submission = pd.read_csv(sample_submission_path)\n","    sub_tokens = sample_submission[\"ÏòÅÏóÖÏùºÏûê\"].astype(str).tolist()\n","    menu_cols = [c for c in sample_submission.columns if c != \"ÏòÅÏóÖÏùºÏûê\"]\n","\n","    # dedup + pivot\n","    dedup = (full_pred_df\n","             .groupby([\"token\",\"ÏòÅÏóÖÏû•Î™Ö_Î©îÎâ¥Î™Ö\"], as_index=False)[\"Îß§Ï∂úÏàòÎüâ\"]\n","             .sum())\n","    wide_pred = dedup.pivot_table(index=\"token\",\n","                                  columns=\"ÏòÅÏóÖÏû•Î™Ö_Î©îÎâ¥Î™Ö\",\n","                                  values=\"Îß§Ï∂úÏàòÎüâ\",\n","                                  aggfunc=\"sum\",\n","                                  fill_value=0.0)\n","\n","    arr = (wide_pred\n","           .reindex(index=sub_tokens, columns=menu_cols, fill_value=0.0)\n","           .astype(\"float64\"))\n","    sub_out = arr.reset_index().rename(columns={\"token\":\"ÏòÅÏóÖÏùºÏûê\"})\n","\n","    vals = sub_out[menu_cols].to_numpy(dtype=\"float64\")\n","    print(\"[SUB] sum:\", float(np.nansum(vals)))\n","    print(\"[SUB] nonzero:\", int(np.count_nonzero(vals)))\n","    print(\"[SUB] zero_ratio:\", float((vals == 0).mean()))\n","\n","    if save_path is not None:\n","        os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n","        sub_out.to_csv(save_path, index=False, encoding=\"utf-8-sig\", float_format=\"%.6f\")\n","        print(\"üíæ Saved:\", save_path)\n","\n","    return sub_out, wide_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXmuKxTiyop4"},"outputs":[],"source":["import itertools\n","def main():\n","    print(\"üì• Loading train:\", TRAIN_PATH)\n","    train_raw = pd.read_csv(TRAIN_PATH)\n","    train_df  = dataPreProcessing(train_raw)\n","\n","    item2idx = build_item_index(train_df)\n","    print(\"items:\", len(item2idx))\n","\n","    print(\"üöÄ Training LSTM...\")\n","    model, best_val = run_train_lstm(train_df, item2idx, EXO_COLS,\n","                                     ctx_len=LOOKBACK, pred_len=PREDICT,\n","                                     batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n","                                     device=DEVICE)\n","    print(f\"‚úÖ Best Val sMAPE: {best_val:.5f}\")\n","\n","    os.makedirs(\"./models/lstm\", exist_ok=True)\n","    torch.save({\"state_dict\": model.state_dict(),\n","                \"item2idx\": item2idx,\n","                \"exo_cols\": EXO_COLS},\n","               \"./models/lstm/best.pt\")\n","    print(\"üíæ Saved model ‚Üí ./models/lstm/best.pt\")\n","\n","# ÌïÑÏöî Ïãú Ìïú Î≤àÎßå ÏàòÌñâÌï¥ÏÑú ckpt ÏÉùÏÑ±:\n","# main()\n","\n"]},{"cell_type":"code","source":["main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmRbL3IUhPfN","outputId":"dd41f6ab-62ce-4514-d9b8-a6dc0715178b","executionInfo":{"status":"ok","timestamp":1755946570129,"user_tz":-540,"elapsed":138415,"user":{"displayName":"gy k","userId":"14114798726788534519"}}},"execution_count":20,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["üì• Loading train: ./train/train.csv\n","items: 193\n","üöÄ Training LSTM...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-2433718288.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 01] train_loss=1.1766  val_sMAPE=1.01173\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 02] train_loss=0.9651  val_sMAPE=1.07008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 03] train_loss=0.8951  val_sMAPE=0.88092\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 04] train_loss=0.8592  val_sMAPE=0.90993\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 05] train_loss=0.8275  val_sMAPE=0.84883\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 06] train_loss=0.7963  val_sMAPE=0.90015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 07] train_loss=0.7759  val_sMAPE=0.75527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 08] train_loss=0.7431  val_sMAPE=0.72614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 09] train_loss=0.7153  val_sMAPE=0.76431\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 10] train_loss=0.6923  val_sMAPE=0.71147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 11] train_loss=0.6587  val_sMAPE=0.74327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 12] train_loss=0.6269  val_sMAPE=0.91077\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 13] train_loss=0.5939  val_sMAPE=0.85796\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[EP 14] train_loss=0.5668  val_sMAPE=0.69661\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[EP 15] train_loss=0.5364  val_sMAPE=0.77138\n","‚úÖ Best Val sMAPE: 0.69661\n","üíæ Saved model ‚Üí ./models/lstm/best.pt\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai63TuenUx1t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1fc7dcb-9e4b-4488-a33d-18cb04d465a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Í≤ÄÏ¶ù Î∏îÎ°ù(Ïª®ÌÖçÏä§Ìä∏ ÎßàÏßÄÎßâ ÎÇ†Ïßú): [datetime.date(2024, 6, 8), datetime.date(2024, 6, 1)]\n","ÌÉêÏÉâ Ï°∞Ìï© Ïàò: 648\n","  progress: 10/648 ...\n","  progress: 20/648 ...\n","  progress: 30/648 ...\n","  progress: 40/648 ...\n","  progress: 50/648 ...\n","  progress: 60/648 ...\n","  progress: 70/648 ...\n","  progress: 80/648 ...\n","  progress: 90/648 ...\n","  progress: 100/648 ...\n","  progress: 110/648 ...\n","  progress: 120/648 ...\n","  progress: 130/648 ...\n","  progress: 140/648 ...\n","  progress: 150/648 ...\n","  progress: 160/648 ...\n","  progress: 170/648 ...\n","  progress: 180/648 ...\n","  progress: 190/648 ...\n","  progress: 200/648 ...\n","  progress: 210/648 ...\n","  progress: 220/648 ...\n","  progress: 230/648 ...\n","  progress: 240/648 ...\n","  progress: 250/648 ...\n","  progress: 260/648 ...\n","  progress: 270/648 ...\n","  progress: 280/648 ...\n","  progress: 290/648 ...\n","  progress: 300/648 ...\n","  progress: 310/648 ...\n","  progress: 320/648 ...\n","  progress: 330/648 ...\n","  progress: 340/648 ...\n","  progress: 350/648 ...\n","  progress: 360/648 ...\n","  progress: 370/648 ...\n","  progress: 380/648 ...\n","  progress: 390/648 ...\n","  progress: 400/648 ...\n","  progress: 410/648 ...\n","  progress: 420/648 ...\n","  progress: 430/648 ...\n","  progress: 440/648 ...\n","  progress: 450/648 ...\n","  progress: 460/648 ...\n","  progress: 470/648 ...\n","  progress: 480/648 ...\n","  progress: 490/648 ...\n","  progress: 500/648 ...\n","  progress: 510/648 ...\n","  progress: 520/648 ...\n","  progress: 530/648 ...\n","  progress: 540/648 ...\n","  progress: 550/648 ...\n","  progress: 560/648 ...\n","  progress: 570/648 ...\n","  progress: 580/648 ...\n"]}],"source":["# ============================================================\n","# Ïû¨ÌïôÏäµ ÏóÜÏù¥ post-process ÏûêÎèô ÌäúÎãù ‚Üí TEST Ïû¨Ï∂îÎ°† & Ï†úÏ∂ú ÏÉùÏÑ±\n","# - train ÎßàÏßÄÎßâ 2~3Í∞ú Î∏îÎ°ùÏùÑ \"Í∞ÄÏßú ÌÖåÏä§Ìä∏\"Î°ú ÎßåÎì§Ïñ¥ sMAPEÎ•º Ï∏°Ï†ï\n","# - ÏÜåÌîÑÌä∏Í≤åÏù¥ÌåÖ/Î∞îÎã•Í∞í/Î¶¨Ïä§ÏºÄÏùº/Í∞êÏá† ÌïòÏù¥ÌçºÎ•º ÏÜåÍ∑úÎ™® Í∑∏Î¶¨ÎìúÏóêÏÑú ÌÉêÏÉâ\n","# - ÏµúÏ†Å Ï°∞Ìï©ÏúºÎ°ú TEST Ïû¨Ï∂îÎ°†Ìï¥ Ï†úÏ∂úÌååÏùº Ï†ÄÏû•\n","# ============================================================\n","import os, glob, numpy as np, pandas as pd, torch, itertools\n","from datetime import timedelta\n","\n","DEVICE       = globals().get(\"DEVICE\", torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","TRAIN_PATH   = globals().get(\"TRAIN_PATH\", \"./train/train.csv\")\n","SAMPLE_SUB   = globals().get(\"SAMPLE_SUB\", \"./sample_submission.csv\")\n","TEST_PATTERN = globals().get(\"TEST_PATTERN\", \"./test/TEST_*.csv\")\n","SUBMIT_DIR   = globals().get(\"SUBMIT_DIR\", \"./submission\")\n","ckpt_path    = \"./models/lstm/best.pt\"\n","os.makedirs(SUBMIT_DIR, exist_ok=True)\n","\n","# ---------- Ïú†Ìã∏ ----------\n","def smape_np(y_true, y_pred, eps=1e-6):\n","    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n","    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n","    y_true = y_true[mask]; y_pred = y_pred[mask]\n","    # NOTE: ÏÇ¨Ïö©ÏûêÍ∞Ä Í≥µÏú†Ìïú Í∑úÏπô( y=0 Ï†úÏô∏ )ÏùÑ Î∞òÏòÅÌïòÎ†§Î©¥ ÏïÑÎûò Ìïú Ï§ÑÏùÑ Ïº≠ÎãàÎã§.\n","    mask2 = (y_true != 0)\n","    y_true = y_true[mask2]; y_pred = y_pred[mask2]\n","    den = (np.abs(y_true) + np.abs(y_pred)).clip(min=eps)\n","    num = np.abs(y_true - y_pred)\n","    return float((2.0 * num / den).mean())\n","\n","def load_model_and_meta():\n","    # Î©îÎ™®Î¶¨ Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ ckpt Î°úÎìú\n","    mdl = globals().get(\"model\", None)\n","    itm2idx = globals().get(\"item2idx\", None)\n","    exo_cols = globals().get(\"EXO_COLS\", [\"weekend\",\"holiday\",\"vacation_season\",\"usage_type_encoded\",\"is_premium\"])\n","    if isinstance(mdl, torch.nn.Module) and itm2idx is not None:\n","        return mdl, itm2idx, exo_cols\n","    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n","    itm2idx = ckpt.get(\"item2idx\")\n","    exo_cols = ckpt.get(\"exo_cols\", exo_cols)\n","    n_items = len(itm2idx) + 1\n","    mdl = Seq2SeqLSTM(n_items=n_items, exo_dim=len(exo_cols)).to(DEVICE)\n","    mdl.load_state_dict(ckpt[\"state_dict\"]); mdl.eval()\n","    return mdl, itm2idx, exo_cols\n","\n","def make_pseudo_block(train_df, end_date):\n","    \"\"\"end_dateÎ•º Ïª®ÌÖçÏä§Ìä∏ ÎßàÏßÄÎßâÏúºÎ°ú ÌïòÎäî 28Ïùº(+ ÎØ∏Îûò7ÏùºÏùÄ ÎπÑÍµêÏö©) Í∞ÄÏßú ÌÖåÏä§Ìä∏ Î∏îÎ°ù ÏÉùÏÑ±\"\"\"\n","    ctx_start = end_date - pd.Timedelta(days=LOOKBACK-1)\n","    ctx = train_df[(train_df['date_time']>=ctx_start) & (train_df['date_time']<=end_date)].copy()\n","    # Î©îÎâ¥Î≥ÑÎ°ú 28ÏùºÏù¥ Î™®ÏûêÎùºÎ©¥ Ï†úÏô∏\n","    counts = ctx.groupby('market_menu')['date_time'].nunique()\n","    good_menus = set(counts[counts==LOOKBACK].index)\n","    ctx = ctx[ctx['market_menu'].isin(good_menus)]\n","    return ctx, end_date + pd.Timedelta(days=1), end_date + pd.Timedelta(days=PREDICT)\n","\n","def eval_combo_on_blocks(train_df, mdl, itm2idx, exo_cols, blocks, combo, apply_decay=True):\n","    scores = []\n","    for end_date in blocks:\n","        ctx_df, val_start, val_end = make_pseudo_block(train_df, end_date)\n","        if ctx_df.empty:\n","            continue\n","\n","        # ÏòàÏ∏° (Ïª®ÌÖçÏä§Ìä∏ 28ÏùºÎßå Í∞ÄÏßÑ DFÎ•º raw_test_dfÎ°ú ÎÑ£Ïùå)\n","        pred_df = forecast_next7_for_block_lstm(\n","            raw_test_df=ctx_df[['date_time','market_menu','sales_amount']],\n","            item2idx=itm2idx, model=mdl, exo_cols=exo_cols,\n","            alpha=combo['alpha'],\n","            global_floor_ratio=combo['g_floor'],\n","            dow_floor_ratio=combo['d_floor'],\n","            pos_eps_prob=combo['pos_prob'],\n","            pos_eps_floor=combo['pos_eps'],\n","            rescale_mode=combo['rescale_mode'],\n","            rescale_clip=combo['rescale_clip'],\n","            device=DEVICE,\n","            block_id=f\"VAL_{end_date.date()}\"\n","        )\n","\n","        # (ÏòµÏÖò) horizon decay\n","        if apply_decay and combo.get('decay', None) is not None:\n","            H_DECAY = combo['decay']\n","            h = pred_df['token'].str.extract(r'\\+(\\d)Ïùº')[0].astype(int)\n","            pred_df['Îß§Ï∂úÏàòÎüâ'] = (pred_df['Îß§Ï∂úÏàòÎüâ'] * h.map(H_DECAY).fillna(1.0)).clip(lower=0)\n","\n","        # Ï†ïÎãµ Ï∂îÏ∂ú\n","        truth = train_df[(train_df['date_time']>=val_start) & (train_df['date_time']<=val_end)]\\\n","                        [['market_menu','date_time','sales_amount']].copy()\n","\n","        # ÏòàÏ∏° DFÎ•º Î≥ëÌï©Ïö© ÌòïÌÉúÎ°ú Ï†ïÎ¶¨\n","        pred_df2 = pred_df.rename(columns={'ÏòÅÏóÖÏû•Î™Ö_Î©îÎâ¥Î™Ö':'market_menu', 'Îß§Ï∂úÏàòÎüâ':'pred'}).copy()\n","        pred_df2['day_offset'] = pred_df2['token'].str.extract(r'\\+(\\d)Ïùº')[0].astype(int)\n","        base_dt = ctx_df['date_time'].max()\n","        pred_df2['date_time'] = (base_dt + pd.to_timedelta(pred_df2['day_offset'], unit='D')).astype('datetime64[ns]')\n","        pred_df2 = pred_df2[['market_menu','date_time','pred']]\n","\n","        # Î≥ëÌï© ÌõÑ sMAPE\n","        merged = truth.merge(pred_df2, on=['market_menu','date_time'], how='inner')\n","        s = smape_np(merged['sales_amount'].to_numpy(), merged['pred'].to_numpy())\n","        scores.append(s)\n","\n","    return float(np.mean(scores)) if len(scores) else 999.0\n","\n","\n","def run_test_inference(mdl, itm2idx, exo_cols, combo, out_name):\n","    files = sorted(glob.glob(TEST_PATTERN))\n","    preds = []\n","    for path in files:\n","        raw = pd.read_csv(path)\n","        block_id = os.path.splitext(os.path.basename(path))[0]\n","        dfp = forecast_next7_for_block_lstm(\n","            raw_test_df=raw, item2idx=itm2idx, model=mdl, exo_cols=exo_cols,\n","            alpha=combo['alpha'],\n","            global_floor_ratio=combo['g_floor'],\n","            dow_floor_ratio=combo['d_floor'],\n","            pos_eps_prob=combo['pos_prob'],\n","            pos_eps_floor=combo['pos_eps'],\n","            rescale_mode=combo['rescale_mode'],\n","            rescale_clip=combo['rescale_clip'],\n","            device=DEVICE, block_id=block_id\n","        )\n","        if combo.get('decay', None) is not None:\n","            H_DECAY = combo['decay']\n","            h = dfp['token'].str.extract(r'\\+(\\d)Ïùº')[0].astype(int)\n","            dfp['Îß§Ï∂úÏàòÎüâ'] = (dfp['Îß§Ï∂úÏàòÎüâ'] * h.map(H_DECAY).fillna(1.0)).clip(lower=0)\n","        preds.append(dfp)\n","    full_pred_df = pd.concat(preds, ignore_index=True)\n","    out_path = os.path.join(SUBMIT_DIR, out_name)\n","    sub_out, _ = build_submission(full_pred_df, SAMPLE_SUB, out_path)\n","    # ÌÜµÍ≥Ñ\n","    menu_cols = [c for c in sub_out.columns if c != \"ÏòÅÏóÖÏùºÏûê\"]\n","    vals = sub_out[menu_cols].to_numpy(float)\n","    print(f\"\\n[{out_name}] sum={np.nansum(vals):,.2f} nonzero={np.count_nonzero(vals):,d} zero_ratio={(vals==0).mean():.4f} \"\n","          f\"median={np.nanmedian(vals):.4f} max={np.nanmax(vals):.2f}\")\n","    return out_path\n","\n","# ---------- Îç∞Ïù¥ÌÑ∞/Î™®Îç∏ Î°úÎìú ----------\n","train_raw = pd.read_csv(TRAIN_PATH)\n","train_df  = dataPreProcessing(train_raw).sort_values(['market_menu','date_time'])\n","mdl, itm2idx, exo_cols = load_model_and_meta()\n","\n","# ---------- ÌäúÎãùÏö© Î∏îÎ°ù ÏÑ†ÌÉù (ÎßàÏßÄÎßâ 14Ïùº, 21Ïùº ÏßÄÏ†ê) ----------\n","last_dt = train_df['date_time'].max()\n","blocks = [last_dt - pd.Timedelta(days=7), last_dt - pd.Timedelta(days=14)]\n","print(\"Í≤ÄÏ¶ù Î∏îÎ°ù(Ïª®ÌÖçÏä§Ìä∏ ÎßàÏßÄÎßâ ÎÇ†Ïßú):\", [b.date() for b in blocks])\n","\n","# ---------- ÌïòÏù¥Ìçº Í∑∏Î¶¨Îìú (ÏûëÍ≤å ÏãúÏûë) ----------\n","alphas   = [0.10, 0.12, 0.15]\n","g_floors = [0.12, 0.16, 0.20]\n","d_floors = [0.08, 0.12, 0.16]\n","pos_probs= [0.45, 0.55]\n","pos_epss = [1.0, 1.2]\n","rescale_modes = ['cross_series']      # crossÎ°ú Í≥†Ï†ï(Í≥ºÎåÄ ÏñµÏ†ú Ïú†Î¶¨)\n","rescale_clips = [(0.8,3.0), (0.8,3.5)]\n","decays = [\n","    None,\n","    {1:1.00, 2:0.997, 3:0.994, 4:0.991, 5:0.988, 6:0.982, 7:0.975},\n","    {1:1.00, 2:0.996, 3:0.992, 4:0.988, 5:0.984, 6:0.976, 7:0.968},\n","]\n","\n","grid = []\n","for a,gf,df,pp,pe,rm,rc,dc in itertools.product(alphas,g_floors,d_floors,pos_probs,pos_epss,rescale_modes,rescale_clips,decays):\n","    grid.append(dict(alpha=a, g_floor=gf, d_floor=df, pos_prob=pp, pos_eps=pe, rescale_mode=rm, rescale_clip=rc, decay=dc))\n","\n","print(f\"ÌÉêÏÉâ Ï°∞Ìï© Ïàò: {len(grid)}\")\n","\n","# ---------- ÌÉêÏÉâ ----------\n","results = []\n","for i, combo in enumerate(grid, 1):\n","    s = eval_combo_on_blocks(train_df, mdl, itm2idx, exo_cols, blocks, combo, apply_decay=True)\n","    results.append((s, combo))\n","    if i % 10 == 0:\n","        print(f\"  progress: {i}/{len(grid)} ...\")\n","\n","results.sort(key=lambda x: x[0])\n","best_s, best_combo = results[0]\n","print(\"\\n=== Top-5 Ï°∞Ìï© (ÏûëÏùÑÏàòÎ°ù Ï¢ãÏùå) ===\")\n","for s, c in results[:5]:\n","    print(f\"sMAPE={s:.4f} | {c}\")\n","\n","print(f\"\\nÏÑ†ÌÉù: sMAPE={best_s:.4f} | {best_combo}\")\n","\n","# ---------- ÏµúÏ†Å ÌïòÏù¥ÌçºÎ°ú TEST Ïû¨Ï∂îÎ°† ----------\n","out_name = \"lstm_submission_autoTuned.csv\"\n","final_path = run_test_inference(mdl, itm2idx, exo_cols, best_combo, out_name)\n","print(\"\\n‚úÖ Ï†úÏ∂ú Ï†ÄÏû•:\", final_path)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["FkI6JV21HdWy"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}